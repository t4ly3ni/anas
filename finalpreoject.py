# -*- coding: utf-8 -*-
"""Untitled44.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cAgjty4dfQpCbN85OsetVsTwRxdI6W_Q
"""

import pandas as pd
from ydata_profiling import ProfileReport
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import RandomizedSearchCV
import joblib

df = pd.read_csv('data/raw/avito_car_dataset_ALL.csv', encoding='latin1')
df.head()

df.info()

df.describe()

df.shape

profile = ProfileReport(df, title='Profiling Report', explorative=True)
profile.to_file("reports/profiling_rep.html")

df.isnull().sum()

# Impute missing values for 'Secteur', 'Origine', 'Première main', 'État' with their mode
for col in ['Secteur', 'Origine', 'Première main', 'État']:
    if df[col].isnull().any():
        mode_value = df[col].mode()[0]
        df[col] = df[col].fillna(mode_value)

# Impute missing values for 'Nombre de portes' with its median
if df['Nombre de portes'].isnull().any():
    median_value = df['Nombre de portes'].median()
    df['Nombre de portes'] = df['Nombre de portes'].fillna(median_value)
df.isnull().sum()

"""## Handle Feature Correlations

### Subtask:
Calculate and visualize the correlation matrix of the features to identify highly correlated variables. This will help in understanding feature relationships and inform decisions on feature selection or engineering.
"""

correlation_matrix = df.corr(numeric_only=True)

plt.figure(figsize=(18, 15))
sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt=".2f", linewidths=.05)
plt.title('Correlation Matrix of Features', fontsize=20)
plt.show()

print("Correlation matrix calculated and visualized.")

# Get the absolute correlation matrix
abs_corr_matrix = correlation_matrix.abs()

# Select upper triangle of correlation matrix to avoid duplicates
upper_tri = abs_corr_matrix.where(np.triu(np.ones(abs_corr_matrix.shape), k=1).astype(bool))

# Find features with correlation greater than a threshold (e.g., 0.7)
highly_correlated_pairs = upper_tri.stack().sort_values(ascending=False)

print("Top highly correlated feature pairs (absolute correlation > 0.7):")
print(highly_correlated_pairs[highly_correlated_pairs > 0.7])

df = df.drop('Airbags', axis=1)
print(" 'Airbags' feature removed.")
df.head()

df.drop_duplicates(inplace=True)

df.duplicated().sum()

numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns
scaler = StandardScaler()
df[numerical_cols] = scaler.fit_transform(df[numerical_cols])
df.describe()

# Determine the number of rows and columns for the subplot grid
num_cols = 4  # Number of columns per row for the plots
num_rows = (len(numerical_cols) + num_cols - 1) // num_cols # Calculate rows needed

plt.figure(figsize=(num_cols * 5, num_rows * 4))

for i, col in enumerate(numerical_cols):
    plt.subplot(num_rows, num_cols, i + 1)
    sns.boxplot(y=df[col])
    plt.title(f'Box Plot of {col}')
    plt.ylabel('') # Remove y-label to avoid clutter
    plt.xlabel('') # Remove x-label to avoid clutter

plt.tight_layout()
plt.show()

def cap_outliers_iqr(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    initial_outliers = len(df[(df[column] < lower_bound) | (df[column] > upper_bound)])
    df[column] = np.where(df[column] < lower_bound, lower_bound, df[column])
    df[column] = np.where(df[column] > upper_bound, upper_bound, df[column])
    return df
for col in numerical_cols:
    df = cap_outliers_iqr(df, col)

num_cols = 4  # Number of columns per row for the plots
num_rows = (len(numerical_cols) + num_cols - 1) // num_cols # Calculate rows needed

plt.figure(figsize=(num_cols * 5, num_rows * 4))

for i, col in enumerate(numerical_cols):
    plt.subplot(num_rows, num_cols, i + 1)
    sns.boxplot(y=df[col])
    plt.title(f'Box Plot of {col}')
    plt.ylabel('') # Remove y-label to avoid clutter
    plt.xlabel('') # Remove x-label to avoid clutter

plt.tight_layout()
plt.show()

categorical_cols = df.select_dtypes(include=['object']).columns
le = LabelEncoder()
for col in categorical_cols:
    df[col] = le.fit_transform(df[col])
df.head()

X = df.drop(['Prix','Unnamed: 0','Lien','Secteur'], axis=1)
y = df['Prix']

print("Features (X) head:")
print(X.head())
print("\nTarget (y) head:")
print(y.head())

"""## Split Data into Training and Testing Sets

### Subtask:
Divide the features (X) and target (y) into training and testing sets using `train_test_split` from `sklearn.model_selection`. This ensures the model is evaluated on unseen data.

## Split Data into Training and Testing Sets

### Subtask:
Divide the features (X) and target (y) into training and testing sets using `train_test_split` from `sklearn.model_selection`. This ensures the model is evaluated on unseen data.
"""


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

"""## Train Supervised Regression Model

### Subtask:
Train a regression model, specifically a `RandomForestRegressor`, using the training data (`X_train` and `y_train`). This step will involve fitting the model to learn the relationship between features and car prices.
"""

# Instantiate the RandomForestRegressor
model = RandomForestRegressor(random_state=44)

# Fit the model to the training data
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

"""## Evaluate Model Performance

### Subtask:
Utilize the trained `RandomForestRegressor` model to predict car prices on the test set (`X_test`). These predictions (`y_pred`) will be compared against the actual car prices (`y_test`) to evaluate the model's accuracy.
"""

mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
print(f"R-squared (R2): {r2:.4f}")



"""# Task
Define a dictionary of hyperparameters and their potential values to be explored by `RandomizedSearchCV` for the `RandomForestRegressor`.

## Define Parameter Grid

### Subtask:
Define a dictionary of hyperparameters and their potential values to be explored by `RandomizedSearchCV` for the `RandomForestRegressor`.

**Reasoning**:
I will define the `param_grid` dictionary with the specified hyperparameters and their potential values for `RandomForestRegressor`, then print it to verify its content.
"""

param_grid = {
    'n_estimators': [100, 200, 300, 400, 500],
    'max_features': ['sqrt', 'log2'],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

print("Parameter grid for RandomizedSearchCV:")
print(param_grid)

"""## Initialize and Fit RandomizedSearchCV

### Subtask:
Initialize `RandomizedSearchCV` with the `RandomForestRegressor`, the defined parameter grid, and a cross-validation strategy. Then, fit it to the training data to find the optimal hyperparameters.

**Reasoning**:
I need to initialize and fit `RandomizedSearchCV` to find the best hyperparameters for the `RandomForestRegressor` model. This involves importing the necessary class, instantiating the regressor, setting up the `RandomizedSearchCV` object with the previously defined `param_grid`, and then fitting it to the training data.
"""


# Instantiate the RandomForestRegressor
rf_model = RandomForestRegressor(random_state=42)

# Initialize RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=rf_model,
    param_distributions=param_grid,
    n_iter=20, # Number of parameter settings that are sampled
    cv=3,       # Number of cross-validation folds
    verbose=2,  # Output progress during the search
    random_state=45,
    n_jobs=-1   # Use all available CPU cores
)

# Fit RandomizedSearchCV to the training data
random_search.fit(X_train, y_train)

# Print the best parameters and best score
print("Best Hyperparameters found:", random_search.best_params_)
print("Best Score (R^2) found:", random_search.best_score_)

"""# Task
Retrieve the best performing model from `RandomizedSearchCV` and use it to make predictions on the test set. Then, calculate and display the evaluation metrics (MAE, MSE, RMSE, R2) to assess its performance. Finally, summarize the model improvement achieved with `RandomizedSearchCV` and compare its performance to the initial model.

## Evaluate Best Model

### Subtask:
Retrieve the best performing model from `RandomizedSearchCV` and use it to make predictions on the test set. Then, calculate and display the evaluation metrics (MAE, MSE, RMSE, R2) to assess its performance.

**Reasoning**:
First, I'll retrieve the best model from the `RandomizedSearchCV` object. Then, I'll use this model to make predictions on the test set and calculate the evaluation metrics (MAE, MSE, RMSE, R2) to assess its performance.
"""

best_model = random_search.best_estimator_

# Make predictions on the test set with the best model
y_pred_tuned = best_model.predict(X_test)

# Calculate evaluation metrics
mae_tuned = mean_absolute_error(y_test, y_pred_tuned)
mse_tuned = mean_squared_error(y_test, y_pred_tuned)
rmse_tuned = np.sqrt(mse_tuned)
r2_tuned = r2_score(y_test, y_pred_tuned)

print("Evaluation Metrics for Tuned Model:")
print(f"Mean Absolute Error (MAE): {mae_tuned:.4f}")
print(f"Mean Squared Error (MSE): {mse_tuned:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse_tuned:.4f}")
print(f"R-squared (R2): {r2_tuned:.4f}")

"""## Summarize Model Improvement

### Subtask:
Summarize the model improvement achieved with `RandomizedSearchCV` and compare its performance to the initial model.

**Reasoning**:
The initial model achieved an R-squared (R2) of 0.9753, a Mean Absolute Error (MAE) of 0.0005, and a Root Mean Squared Error (RMSE) of 0.0012.
The tuned model, after `RandomizedSearchCV`, achieved an R-squared (R2) of 0.9804, an MAE of 0.0004, and an RMSE of 0.0011.

Comparing these metrics, the tuned model shows a slight improvement in all aspects:
- **R-squared (R2)**: Increased from 0.9753 to 0.9804, indicating that the tuned model explains a higher percentage of the variance in the target variable.
- **Mean Absolute Error (MAE)**: Decreased from 0.0005 to 0.0004, meaning the average absolute difference between predicted and actual values is smaller.
- **Root Mean Squared Error (RMSE)**: Decreased from 0.0012 to 0.0011, suggesting that larger errors are penalized less, and the model's predictions are closer to the actual values on average.

Overall, `RandomizedSearchCV` successfully optimized the hyperparameters of the `RandomForestRegressor`, leading to a more robust and accurate model for predicting car prices.

## Final Task

### Subtask:
Summarize the model improvement achieved with `RandomizedSearchCV` and compare its performance to the initial model.

## Summary:

### Q&A
The model improvement achieved with `RandomizedSearchCV` resulted in a more robust and accurate model compared to the initial model.

### Data Analysis Key Findings
*   The tuned model, optimized with `RandomizedSearchCV`, achieved an R-squared (R2) of 0.9804, a Mean Absolute Error (MAE) of 0.0004, and a Root Mean Squared Error (RMSE) of 0.0011.
*   Compared to the initial model, the tuned model showed a performance improvement across all metrics:
    *   R-squared (R2) increased from 0.9753 to 0.9804, indicating a higher percentage of variance explained.
    *   Mean Absolute Error (MAE) decreased from 0.0005 to 0.0004, signifying a reduction in the average absolute prediction error.
    *   Root Mean Squared Error (RMSE) decreased from 0.0012 to 0.0011, implying that the model's predictions are closer to the actual values on average and large errors are penalized less.

### Insights or Next Steps
*   `RandomizedSearchCV` successfully optimized the hyperparameters of the `RandomForestRegressor`, leading to a demonstrably more accurate model for predicting car prices.
*   Further analysis could involve exploring the specific hyperparameters that contributed most to the improvement and considering alternative advanced ensemble methods for potential marginal gains.
"""
joblib.dump(best_model, 'models/car_model.pkl')

# Save a new scaler fitted only on the features used for prediction (not Prix and Unnamed: 0)
# Re-fit the scaler on only the numerical columns that will be in X
numerical_cols_for_prediction = X.select_dtypes(include=['int64', 'float64']).columns
scaler_for_prediction = StandardScaler()
scaler_for_prediction.fit(X[numerical_cols_for_prediction])
joblib.dump(scaler_for_prediction, 'models/scaler.pkl')

# Save feature information for prediction
import json
feature_info = {
    'feature_names': X.columns.tolist(),
    'numerical_cols': numerical_cols_for_prediction.tolist(),
    'categorical_cols': X.select_dtypes(include=['object']).columns.tolist()
}
with open('artifacts/feature_info.json', 'w') as f:
    json.dump(feature_info, f, indent=2)